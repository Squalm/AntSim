<!DOCTYPE html><script src="https://cdn.jsdelivr.net/npm/texme@1.1.0"></script><textarea>
# Abstract
Emergence is a property that we see all the time in systems with simple rules - chaos.  More precisely, unpredictability. We see this all the time in the natural world: ant colonies, birds flocking, and even life itself are all bizarre consequences of rules that we can fully understand. In my EPQ I set out to understand the practical applications of emergence in solving problems we face as well as it's ability to model natural systems that were previously hard to understand.

There are a number of mathematical problems which are particularly hard to solve more efficiently than brute-force, and the problem arises when brute-forcing the solution simply takes too long. A simple example is the travelling salesman problem: a merchant wants to travel in a loop through a series of cities (reaching each only once), what's the shortest distance they must travel? There is a correct answer, but no one has found a way finding it without testing every path. This is where emergence comes in - with simple systems modelled nature we can generate a very good solution in a tiny fraction of the time it would otherwise take. However, the solution we find isn't guaranteed to be the best solution, it is a heuristic, but finding approximate solutions to problems like this is crucial all the time. This same process is, for example, what drives machine learning, which is a crucial part of our modern world.

In my EPQ I aim to investigate a number of different emergent systems - primarily by modelling them myself. Some of the results are beautiful, others are underwhelming but they all offer some insight into problem solving or nature itself.

A note, in the following sections I make a lot of reference to and show a lot of code. I do not expect all readers to fully understand the code, but I have labelled most lines with a description of what they do (shown by a `#` or `#== ... ==#` for multiple lines). I highly recommend the reader attempts to read through all of the code, which should provide a more detailed understanding of the algorithms involved, but if certain snippets are too complicated, it is fine to move on with only the understanding provided by the text explanations.

# Ant Colony Optimisation
Ant colonies have inspired an excellent heuristic for the travelling salesperson problem (TSP)<sup>[12]</sup>. No one has found a practical algorithm to solve this or other similar NP-hard problems but there are a number of reliable heuristics. 

## Final Product
In TSP, the times to solve the problem by brute force (the best known algorithmic solution) get exponentially larger.

*10 points:*
![10 points TSP.png](10%20points%20TSP.png)  
*13 points:*
![13 points TSP.png](13%20points%20TSP.png)  
![14,17 points TSP.png](14,17%20points%20TSP.png)  

As you can see, the estimated time to solve/the actual time required (shown on the right) goes from merely a second for 10 points, to 23 minutes for 13, then 5.5 hours at 14 points and over 2.5 years for 17 points. The point of a good heuristic for TSP is to provide a guess at a solution while making huge savings on time invested.

An Ant Colony Optimisation (ACO) approach to TSP follows a loop:
- Release ants to go between points, they are biased to follow pheromone trails.
- Score how far the ants walked and use that to determine the intensity of pheromones to place along that route.
- Optionally, some simple further optimisation can be performed on the route the ants walked.
- Remove some fraction of the pheromones so the number remain useful.

Applying these steps can produce the following (note the times taken):

ACO usually reached the correct or a solution that was almost identical to the brute force solution at 10 points (and lower):

*10 Points in 0 seconds with ACO (no further optimisation) (2000 iterations, 20 ants):*
![10 Points ACO.png](10%20Points%20ACO.png)  
*Same 10 points in 1 second by Brute Force:*
![10 Points Brute.png](10%20Points%20Brute.png)  

*Pheromone intensity graphed. Y-axis shows the current point, and the X-axis shows the point to move to. The fewer bright points there are, the more certain ACO is of its solution.*
![10 Points Heatmap.png](10%20Points%20Heatmap.png)

At 14 points we can see that ACO usually reaches a solution it is confident in. But not always (shown in the lower example, we can make improvements ourselves), but with some optimisation the approximation is still good:

*14 Points in 1 second with ACO (optimisation) (2000 iterations, 20 ants, small optimisations each iteration):*
![14 Points ACO Confident.png](14%20Points%20ACO%20Confident.png)

*Pheromone intensity:*
![14 Points ACO Heatmap confident.png](14%20Points%20ACO%20Heatmap%20confident.png)

*14 Points in 1 second with ACO (optimisation) (2000 iterations, 20 ants, small optimisations each iteration):*
![14 Points ACO Unsure.png](14%20Points%20ACO%20Unsure.png)
*Pheromone Intensity:*
![14 Points ACO Heatmap Unsure.png](14%20Points%20ACO%20Heatmap%20Unsure.png)

Without subtle optimisation (max, 10 changes) each iteration, the ants do not have enough time to reach a solution after only 2000 iterations:

*20 Points in 1 second with ACO (no optimisation at any stage) (2000 iterations, 20 ants).*
![20 Points ACO.png](20%20Points%20ACO.png)

We can see the uncertainty in the heatmap:
![20 Points heatmap.png](20%20Points%20heatmap.png)

*20 Points in 2 seconds with ACO (optimised) (2,000 iterations, 20 ants).*
![20 Points ACO Optimised.png](20%20Points%20ACO%20Optimised.png)

![20 Points ACO Heatmap Optimised.png](20%20Points%20ACO%20Heatmap%20Optimised.png)

Sometimes the algorithm would slip up with 2,000 iterations at 20 points. So I gave the ants 20,000 iterations (~13 seconds) and have not seen a stupid solution in my tests. This was very exciting and I wanted to push the algorithm further. In the cases where I could see improvements that could be made to the solutions, it was interesting to see that after 20k iterations the ants had still converged on one route. This demonstrates the nature of ACO as a heuristic and not an algorithm: the ants may give you an answer with utter certainty but that does not mean that they are right.

*30 Points, 20k iterations, 20 ants, optimised, in 29 seconds. Initial pheromone intensity 1.0*
![30 Points ACO.png](30%20Points%20ACO.png)
![30 Points ACO Heatmap.png](30%20Points%20ACO%20Heatmap.png)

I raised the number of ants to 50 and the starting pheromone intensity to 10.0. With the larger number of points this should give the ants more opportunity to find potential routes without accidentally ruling them out through lack of numbers.

*30 Points, 50 ants, starting intensity 10.0.*
![30 Points ACO 50 ants.png](30%20Points%20ACO%2050%20ants.png)
![30 Points ACO Heatmap 50 ants.png](30%20Points%20ACO%20Heatmap%2050%20ants.png)

*40 Points otherwise same, in 2 mins 29 secs.*
![40 Points ACO.png](40%20Points%20ACO.png)
![40 Points ACO Heatmap.png](40%20Points%20ACO%20Heatmap.png)  

With these conditions ACO hit it's limit of providing good solutions at 50 points. I don't doubt that by increasing the number of ants and iterations ACO could solve even larger problems. However, I decided to stop pushing here.

*50 Points in 3 mins 54 seconds (without final optimisation pass).*
![50 Points ACO.png](50%20Points%20ACO.png)

*With final pass:*
![50 Points ACO Optimised.png](50%20Points%20ACO%20Optimised.png)

*Pheromone intensities:*
![50 Points Heatmap.png](50%20Points%20Heatmap.png)

## Explanation
The Program can be split into 3 different sections: Brute force, ACO and optimisation. Brute force and ACO are independent but the optimisation requires some initial route to improve upon. I've separated it out from the ACO section to make it clear that it is not necessary for ACO and it moves slightly further from biomimicry to produce a more reliable faster.

### Brute Force
A brute force algorithm is simple: test every possible combination of points and save the best one. We know that this will find the correct solution because we know it is testing every possible order.

```julia
"""
	brute(points::Vector{Vector{Float64}}) -> (order = Vector{Int}, best_score = Float64)

Uses brute force to find the shortest route through `points`, returning to the starting point.  
Since it does not matter where we start, the first and last points are both 1.
"""
function brute(points::Vector{Vector{Float64}})

	# A variable which will store the distance of 
	# the shortest route the algorithm has found.
	best_score = Inf

	# A Vector which will store the best route.
	best = Int[]

	# For each possible order...
	# (assuming the order starts and ends at 1)
	for order in ProgressBar(permutations(2:length(points), length(points) - 1))

		# Calculate the distance of the order
		distance = route_s(points, order)

		# If the distance is shorter than the current best...
		if distance < best_score

			# Save the new distance and best order
			best_score = distance
			best = deepcopy(order)

		end # if

	end # for

	# Add the first and last point 1 (which was assumed)
	prepend!(best, 1)
	append!(best, 1)
	
	return (order = best, distance = best_score)

end # function
```

### ACO
Each iteration of ACO for TSP begins by generating a number of routes through the points equal to the number of theoretical ants being released. At each decision in the routes, where the ant goes is biased only by the current pheromone intensity pointing in that direction. Otherwise the decision is random. Then the program scores each route by its distance and adds pheromones to the trail in the following relationship:
$$ Ph_{route} = \left( \frac {1} {S} \right) ^ G $$
Where $G$ is the greed of the system and $S$ is the total distance of the route.

Then there is some amount of evaporation. The Pheromone intensity at each point is reduced by a factor E. Since the fraction of pheromone being taken away is constant everywhere, this ensures that the pheromone intensity can never increase above a certain limit and it reduces the intensity through routes that are never used. 

This cycle repeats for the specified number of iterations. 

```julia
"""
	ants(points, [iters, n, evaporation, greed, optimise]) -> (order, distance, ph)

Biomimicry with ant colonisation!  
Returns a route by sending `n` ants around in a loop.
"""
function ants(points::Vector{Vector{Float64}}, iters = 500, n = 20, evaporation = 0.1, greed = 2.0, optimise = true)

	# Create a matrix which will store the pheromone intensities.
	ph = fill(10.0/length(points), length(points), length(points) - 1)

	# Begin iterating...
	for _ in ProgressBar(1:iters)

		# Generate n routes through the points using the pheromone values.
		orders = route(points, ph, n)

		# If each route should be optimised slightly...
		if optimise

			# For each of the suggested orders...
			for order in orders

				# Run optimisation (decrosses most crossed lines)
				# on each order, limited to making 3 changes.
				order = decross(order, points, 3)

			end # for

		end # if

		# Calculate the distances of each route.
		distances = route_s(points, orders)

		# Place pheronmones on each route using the distances.
		ph += lay_ph(orders, distances, points, greed)

		# Allow some fraction of the pheromones to evaporate
		# The fraction is constant in all places.
		ph = (1-evaporation) .* ph

	end # for

	# Save the final pheronomes
	ph_out = deepcopy(ph)

	#== Find the preferred route ==#

	# An array to store the preferred route.
	best = Int[0 for _ in 2:length(points)]

	# A variable to store the current location of the
	# ant which follows the preferred route.
	current = 1

	# For each stop the ant must make...
	for i in 1:length(points)-1

		# Set the pheromones intensities of points 
		# the ant has already visited to -Infinity.
		ph[current, :] = [x+1 in best ? -Inf : ph[current, x] 
						  for x in 1:length(ph[current, :])]
		
		# Find the point with the highest pheromone intensity.
		p = findmax(ph[current, :])[2] + 1

		# Save this point.
		best[i] = p

		# Move the ant to the new point.
		current = p

	end # for

	# Add 1s to the beginning and end of route (assumed).
	prepend!(best, 1)
	append!(best, 1)
	
	return (order = best, distance = route_s(points, best[2:end-1]), ph = ph_out)

end # function
```

#### Generating Routes
In order to send virtual ants around through the points we need to generate routes. This function generates one route for each of the ants. I use weighted sampling to pick a point.

```julia
"""
	route(points, ph, n) -> Vector{Vector{Int64}}

Generate `n` routes through `points` where the pheromone intensities are `ph`.  
Uses weighted random sampling.
"""
function route(points::Vector{Vector{Float64}}, ph::Matrix{Float64}, n::Int)

	# Create a varibale which will hold the routes
	orders = Vector{Int64}[]

	# For each route that needs to be created...
	for _ in 1:n

		#== Get a route ==#
		
		# Create a variable which will temporarily store the route.
		order = Int[0 for _ in 2:length(points)]

		# Store the point where the ant currently is.
		current = 1

		# For each point the ant needs to travel through...
		for i in 1:length(points)-1

			# Store possible points the ant can go to
			# which is the list of points excluding the ones already visited.
			_Ps = [x for x in 2:length(points) if !(x in order)]
			
			# Store the Pheromone values at those point from the current point.
			_Phs = [ph[current, x-1] for x in 2:length(points) if !(x in order)]

			# Pick the next point randomly (weighted by the pheronome trails).
			p = sample(_Ps, pweights(_Phs), 1)[1]

			# Save that new point.
			order[i] = p

			# Move the ant to the new point.
			current = p

		end # for

		# Store the route in the list of routes.
		push!(orders, order)

	end # for

	return orders

end # function
```

#### Calculating Distances

The simulation calculates the distance of each route on request. It would be absurd to calculate the distance of each route ahead of performing ACO since calculating each distance solves TSP anyway. However, I could have calculated a table of distances between points ahead of releasing the ants. I found that the program ran fast enough that this was not a significant issue so I did not implement it. In a larger scale implementation that was required to run faster this should be implemented.

```julia
"""
	route_s(points, order) -> Float64

Returns the distance of the route `order` through `points`
(in n-dimensions).
"""
function route_s(points::Vector{Vector{Float64}}, order::Vector{Int})

	# Create a variable to store the distance.
	distance = 0.0

	# For each point in the route (plus an end point)...
	for i in 1:length(order)+1

		#==
		Add the distance between the previous point
		(assumed starts at 1) and then next point to
		the distance variable.
		==#
		distance += s(points[get(order, i-1, 1)], points[get(order, i, 1)])

	end # for

	return distance
	
end # function
```

#### Laying Pheromones

The pheromones are laid as described above.
$$ Ph = \left( \frac 1 S \right) ^ G $$
I found that a good greed value was 2. The greed value changes the amount which the program is drawn towards routes it found with a low distance. The real life scenario this model is based off would have a greed value of 1. The amount of pheromones along a route with real ants is equal to the number of ants which could follow along the route in a given time. If the route is shorter, the ants can use it more times, so the pheromones will be more intense.
$$ f = \frac v {\lambda} $$
Where $f$ is frequency, $v$ is speed, and $\lambda$ is the distance.

```julia
"""
	lay_ph(orders, distances, points, greed) -> Matrix{Float64}

Returns a matrix like ph which shows the new pheromones laid down in orders.
"""
function lay_ph(orders::Vector{Vector{Int}}, distances::Vector{Float64}, points::Vector{Vector{Float64}}, greed::Float64)

	# Create an empty matrix where the new pheromones can be stored.
	empty = fill(0.0, length(points), length(points) - 1)

	# For each order...
	for j in 1:length(orders)

		#== Update the pheromones ==#

		# Create a variable to store the previous point.
		prev = 1

		# For each point in the route...
		for i in orders[j]

			# Add pheromones to the path between the
			# previous point and the next point.
			empty[prev, i-1] += (1/distances[j]) ^greed

			# Save the current point as the previous point.
			prev = i
		
		end # for

	end # for

	return empty
	
end
```

### Optimisation
The job of my separate optimisation algorithm is to improve the efficiency of ACO by stopping the ants from following trivially stupid routes with easy optimisations. The simple optimisation I applied was to uncross crossed lines, which would always reduce the length of the route.

Reversing the order of points in the loop of a cross always uncrosses the lines and always reduces the length of the route.

![Crossing Explanation](crosses%20explanation.png)

```julia
"""
	decross(order, points, [limit]) -> order::Vector{Int}

Return an order were `limit` many changes were 
made to remove cross lines from the `order` through
`points`.
"""
function decross(order, points, limit = 100)

	# Create a variable we can test later to see 
	# if the algorithm made a change.
	success = false

	#==
	This function is recursive but it needs to 
	know to stop at some limit. This also allows
	us to stop the algorithm making too many changes.
	==#
	if limit > 0

		# Iterate through all the individual paths between points
		# in each route.
		for i in 0:length(order)
			for j in i+2:length(order)

					# Avoid a special case where the algorithm makes
					# the same change forever.
					if !(i == 0 && j == length(order))

						# Find the relevant 4 points.
						# The first line is p1q1.
						p1 = points[get(order, i, 1)]
						q1 = points[get(order, i+1, 1)]

						# The second line is p2q2.
						p2 = points[get(order, j, 1)]
						q2 = points[get(order, j+1, 1)]

						# If the two lines intersect...
						if ifintersect(p1, q1, p2, q2)
													
							# Remove the cross by reflecting the order
							# of points in the loop of the intersection. 
							order[i+1:j] = reverse(order[i+1:j])
							
							# Track that there was a success.
							success = true

							# break out of the loop.
							break
			
						end # if

					end # if

			end # for
			if success
				break
			end # if
		end # for

	end # if

	# If one pair of lines was uncrossed...
	if success

		#==
		Run the algorithm again on the new order.
		Since news crosses may have been introduced by
		the uncrossing.
		==#
		return decross(order, points, limit-1)

	# Otherwise...
	else

		# End the recursion and return the final order.
		return order

	end # if

end # function
```

#### Finding Intersections
A set of 3 points can be described as clockwise or anticlockwise as defined below. In the most general case, the lines p1q1 and p2q2 intersect if the orientations of p1,q1,p2 and p1,q1,q2 are different and the orientations of p2,q2,p1 and p2,q2,q1 are different<sup>[15]</sup>.

There are special cases where three of the points are colinear (lie on the same line). If p,r,q is colinear (and the lines are pq, rs), then if r lies on pq then there is an intersection, but it may simply lie on the same line and not intersect.

![Orientation Explanation](orientation%20explanation.png)

```julia
"""
	ifintersect(p1, q1, p2, q2) -> bool

Checks if p1q1 and p2q2 intersect.
"""
function ifintersect(p1, q1, p2, q2)

	# Find the 4 relevant orientations
	o1 = orientation(p1, q1, p2)
	o2 = orientation(p1, q1, q2)
	o3 = orientation(p2, q2, p1)
	o4 = orientation(p2, q2, q1)

	# If the orientations of the two pairs are different
	# then the lines intersect.
	if (o1 != o2) && (o3 != o4)
		return true
	end # if

	# If any of the Special colinear cases...
	if (o1 == 0 && colinearOnSegment(p1, p2, q1)) ||
			(o2 == 0 && colinearOnSegment(p1, q2, q1)) ||
			(o3 == 0 && colinearOnSegment(p2, p1, q2)) ||
			(o4 == 0 && colinearOnSegment(p2, q1, q2))
		return true
	end # if

	# Otherwise the lines do not intersect.
	return false

end # function
```

#### Orientation
Here we need to calculate if the points are clockwise or anticlockwise. If the gradient between the first two points is less than the gradient between the second two (the third point moves left) then the orientation is anticlockwise. Similarly, if the gradient between the first two points is greater than the gradient between the second two (the third point moves right) then the orientation is clockwise. If the gradients are all the same, then the points are colinear.

![Gradients Explanation](gradients%20explanation.png)

For lines pq and rs:
$$ m = \frac {dy} {dx}(pq) = \frac {q_y - p_y} {q_x - p_x}  $$
$$ n = \frac {dy} {dx} (qr) = \frac {r_y - q_y} {r_x - q_x} $$
If $m > n$, then the points go clockwise, since the second gradient is shallower. If $n > m$, then the points go anticlockwise. The equation which expresses this most easily can be derived as follows:
$$ m - n = \frac {q_y - p_y} {q_x - p_x} - \frac {r_y - q_y} {r_x - q_x} $$
Multiplying by $(p_x-q_x)(q_x-r_x)$:
$$ (q_y-p_y)(r_x-q_x) - (q_x-p_x)(r_y-q_y) $$
This preserves the signs and this function will give a value >=0 when the points go clockwise, a value of 0 when they are colinear, and a value <=0 when they go anticlockwise. The only necessary final step in the code is to lump together all clockwise and anticlockwise cases into having one value.

```julia
"""
	orientation(p, q, r) -> Int

Returns `1` if the points go clockwise, `-1` if Anticlockwise
and `0` if they are colinear.
"""
function orientation(p, q, r)

	# This calculation gives a value that is determined
	# by the orientation (direction) of the points.
	dir = ((q[2] - p[2]) * (r[1] - q[1])) - ((q[1] - p[1]) * (r[2] - q[2]))

	# If the direction is greater than 0...
	if dir > 0

		# Then the points go clockwise.
		return 1

	# If the direction is less than 0...
	elseif dir < 0

		# Then the points go anticlockwise.
		return -1

	# Otherwise the direction is 0
	else

		# The points are colinear.
		return 0
		
	end # if

end # function
```

#### Special Cases
This case only applies when we already know that the point q lies on the same line as pr but we don't know if it lies on the same segment, which would tell us if there is a colinear intersection. This case is very rare in randomly generated points but it is worth including.

If q is on the segment, then the following must be true of its coordinates (which, combined, are equivalent to the if statement in the code below):
$$ \min (p_x, r_x) \leq q_x \leq \max(p_x, r_x) $$
$$ \min(p_y, r_y) \leq q_y \leq \max(p_y, r_y) $$

```julia
"""
	colinearOnSegment(p, q, r) -> bool

Given the points are colinear, checks if q lies on pr.
"""
function colinearOnSegment(p, q, r)

	# Knowing that q lies on the same line pr,
	# check if it is between them or not on the segment...
	if q[1] <= max(p[1], r[1]) && q[1] >= min(p[1], r[1]) && 
			q[2] <= max(p[2], r[2]) && q[2] >= min(p[2], r[2])
		return true
	end # if
	
	return false

end # function

```

## Development
In addition to the code above, I wrote 4 small functions: the first one to generate the points, the second to calculate the distance between two points and the final two to visualise the routes and pheromones after the calculations. Find these In Appendix A.

### Prototype
My first prototype was functional, although it lacked a number improvements I would make later. Only one ant was sent out each iteration and the trails did not evaporation. There also was no optimisation to avoid crossing lines. I did see some good results (see below) but they were not as consistent and the overall structure was much more clunky. Initially I also used a (negative) exponential function to lay the pheromones, this is because I was worried about the 1/x function being too sharp. At the time I think that this was a valid concern because I only used one ant but it did not reflect the system the program was based off.
![13 Points ACO Early.png](13%20Points%20ACO%20Early.png)

### Modifications
Throughout development I experimented with a number of different starting pheromone intensities and methods of laying down pheromones. Eventually I would settle into the system above (which was also the simplest). However, before I reached that conclusion, I tried to improve the efficiency of the algorithm by weighting the starting pheromone intensities by the distance between the two points and by multiplying teh pheromones to be added to each area by 1/ the distance of the path between the two points. In a way, these improved the program, but not in real way. Both changes moved the program towards a nearest-neighbour approach which is another much simpler heuristic for TSP. Eventually i would realise that the laying pheromones and starting conditions were not what was limiting the quality of my program.

In the next version I made most of the changes to make the program into its final state. I cleaned up the code and broke it into more convenient functions which could run for multiple ants and I added the ability to plot the pheromone intensities on a heatmap. I also added the whole separate optimisation algorithm which went through some iteration to reach a state where it worked as intended. The most interesting part of this involving finding the best algorithm to uncross the lines. Initially I thought to swap the locations of the two end points of the cross in the list. I think that repeating this process would eventually uncross all the lines, but it would require many more steps than the final reflection idea I came to later by drawing out a number of crossed lines with points on paper and figuring out how I intuitively wanted to uncross them and what this would do to the list of points.

## Information
Language used: Julia  
Packages imported: ProgressBars, Plots, Combinatorics, StatsBase

---
# Boids
Boids are agents which follow a few simple rules designed to simulate flocking behaviour in birds, herd animals or fish.

## Final Product
The goal was to introduce boids into a space and let them fly around. With the right parameters, they should exhibit flocking behaviour. Everything I did for this section was 2D. However, the same principles can (and have) been applied to 3D.

Each boid follows these 3 rules:
- Separation - don't bump into other boids.
- Cohesion - move towards the centre of mass of the boids around you.
- Alignment - match the speed and direction of boids around you.

There are also two extra rules I needed to implement:
- Keep inside the bounds of the plot.
- Keep speed under a maximum speed.

When applied together we get the following:

*Under normal conditions. (80 boids)*
![220409 BOIDS.gif](220409%20BOIDS.gif)

*With a higher vision radius (50 boids).*
![220423 BOIDS.gif](220423%20BOIDS.gif)

*With extremely high separation.*
![220409 BOIDS jitter.gif](220409%20BOIDS%20jitter.gif)

*With 500 boids.*
![220409 BOIDS 500.gif](220409%20BOIDS%20500.gif)

## Explanation
The program works by applying the functions described below in sequence as follows:

```julia
anim = @animate for _ in ProgressBar(1:steps)

    global boids

    boids = cohesion.(boids)
    boids = separation.(boids)
    boids = alignment.(boids)
    boids = keepin.(boids, w, h)
    boids = limit.(boids, speedlimit)
        
    boids = update.(boids, delta)

    # Pull out the x and y values to be plotted.
    x = [boid[1] for boid in boids]
    y = [boid[2] for boid in boids]

    # Plot the positions of all the boids on a scatter graph.
    scatter(x, y, xlims=(0, w), ylims=(0, h))

end # for
```

### Limits
Two functions are required: one to keep the boids within the edges of the plot, and another to limit the speed of the boids to some reasonable value.

```julia
"""
    keepin(boid, w, h, [edgeforce]) -> boid::Vector{Float64}

Returns the boid, with force applied to keep it inside a box
with width `w` and height `h`.
"""
function keepin(boid, w, h, edgeforce = 1)

    # Define a margin within which boids will 
    # be pushed away from the edge.  
    margin = 20

    #== X values ==#
    # If the boid is within the margin of the edge on the left...
    if boid[1] < margin

        # Push it right using a linear function getting
        # greater as it moves left.
        boid[3] += edgeforce * (margin - boid[1])
    
    # If the boid is within the margin of the edge on the right...
    elseif boid[1] > w-margin

        # Push it left using the same function.
        boid[3] -= edgeforce * (boid[1] - (w - margin))
    
    end # if

    #== Y values ==#
    # The same but substituting left for bottom,
    # and right for top.
    if boid[2] < margin
        boid[4] += edgeforce * (margin - boid[2])
    elseif boid[2] > h-margin
        boid[4] -= edgeforce * (boid[2] - (h - margin))
    end # if

    return boid

end

"""
    limit(boid, speedlimit) -> boid::Vector{Float64}

Returns the boid where the overall speed is capped 
at the speedlimit (direction is preserved).
"""
function limit(boid, speedlimit)
    
    # Calculate the 2D speed of the boid.
    speed = sqrt(boid[3]^2 + boid[4]^2)

    # If the speed is greater than the speedlimit...
    if speed > speedlimit

        # Modify the velocities so that the direction
        # is the same but the speed is the spedlimit.
        boid[3] = boid[3] * speedlimit/speed
        boid[4] = boid[4] * speedlimit/speed

    end # if

    return boid
    
end
```

### Separation
For separation, we simply need to find the closest boid (within the vision radius) and then push the current boid away from it using a $1/x$ function.

The lines where I update the boid's velocity require some explanation. The boid's velocity is stored with separate x and y components. However, the push function needs to know the distance to the closest boid using Euclidean distance in 2D, otherwise two boids at opposite sides of the area could be pushed away from each other since they may still have very similar y coordinates if not x coordinates. To account for this, I first find the distance to the nearest boid, and use that to determine how hard the boid should be pushed, and then split that into x and y components to be applied.
$$ V_x = c * \frac {S_x} {S} * \frac 1 S = \frac {cS_x} {S^2}$$
Here, $S$ is the total distance. Which means $\frac {S_x} S$ is the ratio between the total distance and the $x$ displacement, or the fraction of the total displacement that is in the $x$ direction. The same can be applied to $V_y$ by replacing $S_x$ with $S_y$. N.B., $S_x + S_y \neq S$ so the two fractions will not sum to $1$.

```julia
"""
    separation(boid, [boidpush]) -> boid::Vector{Float64}

Returns the current boid with velocity changed to be pushed
away from the closest boid to it (with a 1/x relationship).  
"""
function separation(boid::Vector{Float64}, boidpush = 5.0)

    # Get all the nearby boids along with how far away they are.
    n = nearby(boid[1:2], boids, vision)

    #==
	We do not want the current boid to try and avoid itself,
	so I am careful to remove it. 
	==#
    n = [x for x in n if s(x[1][1:2], boid[1:2]) > 0]

    # If the current boid can see any boids at all...
    if length(n) > 0

        # Extract the distances from the list of boids and distances.
        near = [x[1] for x in n]

        # Find the location of the nearest boid.
        closest = near[findmin([x[2] for x in n])[2]][1:2]

        # Find the distance from the current boid to the nearest boid.
        distance = s(closest, boid[1:2])

        #== 
        Change the velocity of the current boid by using the trig
        relationship between side lengths and the 1/distance function
        multiplied by some constant (boidpush). 
        ==#
        boid[3] += boidpush * (boid[1] - closest[1]) / distance^2
        boid[4] += boidpush * (boid[2] - closest[2]) / distance^2

    end # if

    return boid

end
```

Here you can see the boids avoiding each other. $\frac 1 x$ is quite a strong force, so they move quite quickly.
![220418 BOIDS separation.gif](220418%20BOIDS%20separation.gif)

### Cohesion
For cohesion, we just need to apply a small force which moves boids towards the centre of other boids it can see. This mimics the jostling for the easy position in the centre of flocks seen in birds.

```julia
"""
    cohesion(boid, [centerpull]) -> boid::Vector{Float64}

Returns the current boid with velocity changed to be pulled
towards the centre of mass of boids it can see.
"""
function cohesion(boid::Vector{Float64}, centerpull = 0.08)

    # Initialise an empty variable that can hold [x, y] coordinates.
    center = Float64[0.0, 0.0]

    # Get the locations of the boids the current boid can see.
    near = [x[1] for x in nearby(boid[1:2], boids, vision)]

    #==
    Iterate through the list of nearby boids:
    For each boid, add the x and y coordinates to
    the x and y parts of center.
    ==#
    for b in near
        center[1] += b[1]
        center[2] += b[2]
    end # for

    # If there are any boids nearby...
    if length(near) > 0

        # Divide the center by the number of boids nearby, 
        # to give an average value
        center ./= length(near)

        #==
        Take the difference between the center and the current boid
        and multipy that by some constant. Then apply each
        component to current boid's velocity.
        ==#
        boid[3] += (center[1] - boid[1]) * centerpull
        boid[4] += (center[2] - boid[2]) * centerpull
    
    end # if

    return boid

end
```

Here you can see the boids forming into groups circling around the centres. As the vision radius is increased they form these groups much faster.
![220418 BOIDS cohesion.gif](220418%20BOIDS%20cohesion.gif)

### Alignment
For alignment, we need to find the average speed and direction (or rather, average velocities in x and y) and move the current boid's velocity towards that.

```julia
"""
    alignment(boid, [turnfactor]) -> boid::Vector{Float64}

Returns the current boid with velocity changed to move towards
the same speed and direction of the boids around it.
"""
function alignment(boid::Vector{Float64}, turnFactor = 0.1)

    # Initialise a variable that can hold the average values.
    avg = [0.0, 0.0]

    near = [x[1] for x in nearby(boid[1:2], boids, vision)]
    
    #==
    Iterate through the list of nearby boids:
    For each boid, add the V_x and V_y coordinates to
    the V_x and V_y parts of the average.
    ==#
    for b in near
        avg[1] += b[3]
        avg[2] += b[4]
    end # for

    if length(near) > 0

        # Divide the sum by the number of boids nearby, 
        # to give an average value.
        avg ./= length(near)

        # Move towards the average speeds by some factor.
        boid[3] += avg[1] * turnFactor
        boid[4] += avg[2] * turnFactor

    end # if

    return boid

end
```

Here you can see the boids aligning with groups of other boids. You can see how some boids occasionally align with a group they can see but are actually flung away from it, this demonstrates the need for a cohesive force.
![220418 BOIDS alignment.gif](220418%20BOIDS%20alignment.gif)

## Development
Additionally to what is described above, a few functions are necessary. Find these in Appendix B. 

### Prototype

In my first prototype, the `keepin`, `nearby`, `limit` and `update` functions would remain unchanged in the final version.

In my tests, I had observed that using a $1/x$ function for separation would fling the boids too quickly away from each other so the effects of the other forces would be barely visible. Instead I had decided to use a sigmoid derivative. This did not work because the sigmoid derivative is not signed; it is positive on both sides so the function could not tell from which side the boids were approaching and it would always push right/up. This also did not work because the function was not strong enough.

Despite this, the program produced not unrecognisable behaviour:
![220316 BOIDS.gif](220316%20BOIDS.gif)

### Modifications
The next steps in improving the model were changing what functions were being used for each of the different forces. I tried using an exponential at the edges for a slower turning effect. I thought this might help because it would allow the boids to be more coherent as they hit the edge. This did not help (the force was too strong). I updated the sigmoid derivative in the separation function to be signed, but this still did not fix the underlying issue which I address in the separation section above: the functions did not care about real distance between boids, only the x and y components. I also experimented with the size of the area and the number of boids to see what would be best.

In the next versions, I was still experimenting with functions in the forces. In separation, I tried using the average centre and repelling the boid from that using 1/x. In retrospect, this obviously does not make sense since this is precisely the opposite of the cohesion function. The critical realisation that I would later have is that it would be much better for me to simply repel the boid only from the single nearest boid to it.

After a few further small modifications and quality of life improvements I had produced the model described above.

## Information
Language used: Julia  
Packages imported: ProgressBars, Plots

---
# Reaction Diffusion
This simple (albeit slightly unrealistic) spatial simulation of a chemical reaction demonstrates clearly the unpredictability of making very small changes to the starting conditions of a chaotic system (that may be emergent).

## Final Product
A Reaction diffusion simulation is a simulated reaction between two chemicals, A and B (some sources will use U and V). Four steps happen every tick of the reaction:

- A and B diffuse a little into their surroundings.
- Two Bs and an A can react to form three Bs (BBA -> BBB).
- Some A is added.
- Some B is removed.

The simulation works by approximating the chemicals in a grid of cells each of which contains between 0.0 and 1.0 of A and the same of B representing the portion of the total max A or B that can fit in each cell.

When these rules are applied, we can see myriad different emergent behaviours by plotting the amount of A:

*Varying the feed rate along the x axis and the kill rate along the y axis.*
![220416 REACT varing f and k.png](220416%20REACT%20varing%20f%20and%20k.png)

*Growth (network): Feed = 0.055, kill = 0.062*
![220416 REACT.gif](220416%20REACT.gif)

*Growth (without connection): Feed = 0.05, kill = 0.065*
![220416 REACT lonk.gif](220416%20REACT%20lonk.gif)

*Mitosis: Feed = 0.05, kill = 0.066*
![220418 REACT mitosis.gif](220418%20REACT%20mitosis.gif)

## Explanation
The model creates a gif of the reaction by capturing every 40th iteration and playing it back at 30 fps.

```julia
# For each frame in the cache...
anim = @animate for frame in ProgressBar(cache.A)

	# Create a heatmap showing the amount of A.
	heatmap(frame, clim=(0,1), c = :gist_rainbow)
	
end # for

# Create a gif of all the heatmaps.
gif(anim, "reaction"*string(loop)*".gif", fps = 30)
```

### Initialisation
The initial state and properties of the simulation are particularly important in this case. I use a number of starting patches of B. If there is any order in these placements, then this order is generally shown at the end of the simulation so I use random placement. This has been quite effective at creating organic structures.

```julia
# Constants
dims = 200
steps = 1000
interval = 40

# Variables for storing the state.
A = ones(Float64, dims, dims)
A2 = ones(Float64, dims, dims)
B = zeros(Float64, dims, dims)
B2 = zeros(Float64, dims, dims)

# Seed B
for _ in 1:dims
    B[max(1, trunc(Int, rand() * dims)), max(1, trunc(Int, rand() * dims))] = 0.9
end # for

# Setup caches
cache = (A = [zeros(Float64, dims, dims) for _ in 1:trunc(Int, steps/interval)], 
         B = [zeros(Float64, dims, dims) for _ in 1:trunc(Int, steps/interval)])

# Global constants
global feed = 0.055 
global kill = 0.062
global D_A = 1.0
global D_B = 0.5
global delta = 1.0
```

### Diffusion
For diffusion, we need to find some function that will get the amount of a chemical after diffusion that won't change the total amount of a chemical after this has been applied to all cells. For this I used a Laplacian function which calculates where we would expect fluid to collect in a field.

```julia
"""
    diffuse(coords, C) -> Float64

Returns the amount of chemical C (where `C` is a 2D matrix)
at `coords` after some diffusion.
"""
function diffuse(coords::Tuple, C)

	#== 
    In order to calculate the new value for this cell:
    Create a weighted average of all the cells around it,
    (orthogonally adjacent cells have a weight of 0.2 and 
    diagonally adjacent cells have a weight of 0.05).
    Then subtract the value in the current cell.
    If any of the adjacent cells don't exist, assume
    they have the value of the current cell.
    ==#
    return sum(
        get(C, (coords[1] + 1, coords[2]), C[coords[1], coords[2]]) *0.2 +
        get(C, (coords[1] - 1, coords[2]), C[coords[1], coords[2]]) *0.2 +
        get(C, (coords[1], coords[2] + 1), C[coords[1], coords[2]]) *0.2 +
        get(C, (coords[1], coords[2] - 1), C[coords[1], coords[2]]) *0.2 +
        get(C, (coords[1] + 1, coords[2] + 1), C[coords[1], coords[2]]) *0.05 +
        get(C, (coords[1] - 1, coords[2] + 1), C[coords[1], coords[2]]) *0.05 +
        get(C, (coords[1] - 1, coords[2] - 1), C[coords[1], coords[2]]) *0.05 +
        get(C, (coords[1] + 1, coords[2] - 1), C[coords[1], coords[2]]) *0.05 +
        get(C, coords, 1.0) * (-1.0)
    )

end
```

### Simulation
Storing all the data from long simulations requires too much memory, so I break the whole task up into fragments which each generate a gif and then I can stitch those together to get the whole simulation.

Since the A and B values are between 0 and 1 they lend themselves nicely to being treated as probabilities. If the amount of A is 1.0 then we are certain to be able to find some A to react, but if there is only 0.1 B then very little reaction will occur, because 2 Bs are required. In this vein, we use $ABB$ as the amount of A that will react and become B.

To calculate the new amount of A at a coordinate we use the following:
$$ A'_{x,y} = A_{x,y} + \left(D_A \nabla \cdot A_{x,y} - A_{x,y}B^2_{x,y} + F(1-A_{x,y})\right) \delta t$$
Where $D_A$ is some constant ($1.0$), $\nabla \cdot A_{x,y}$ is the diffused A value, $F$ is the feed rate and $\delta t$ is the time interval. $A_{x,y}B^2_{x,y}$ is the amount of A that is going to react (and become B, so it is subtracted). $F$ is multiplied by $(1-A_{x,y})$ which prevents $A_{x,y}$ from ever going over 1.

To calculate the new amount of B at a coordinate we use the following:
$$ B'_{x,y} = B_{x,y} + \left(D_B \nabla \cdot B_{x,y} + A_{x,y}B^2_{x,y} - (K + F)B_{x,y} \right) \delta t$$
Where $D_B$ is some constant ($0.5$), $\nabla \cdot B_{x,y}$ is the diffused B value, $F$ is the feed rate, $K$ is the kill rate and $\delta t$ is the time interval. In this case, $A_{x,y}B^2_{x,y}$ is added because the A becomes B. $(K+F)$ is multiplied by $B_{x,y}$ so that $B_{x,y}$ never goes below 0 and so that the amount that is removed is proportional to the amount that is present.

```julia
# Looping in order to break up the task.
for loop in 1:20

    # Iterate for 'steps' many steps...
    for i in ProgressBar(1:steps)

        # Disambiguate access to A and B we defined above
        global A
        global B

        # For each x coordinate (columns)...
        Threads.@threads for x in 1:dims

            #kill = 0.05 + x/dims * 0.02
            
            # For each y coordinate (now rows)...
            # This therefore goes through every cell.
            for y in 1:dims

                #feed = 0.04 + y/dims * 0.04

                # Calculate the amount of ABB which will become BBB.
                ABB = A[x, y]*B[x, y]*B[x, y]

                #==
                Calculate the amount of A that should be at these coordinates
                and store it in A2 (so that all the new values are calculated
                before the old ones are used.
                ==#
                A2[x, y] = A[x, y] + (D_A * diffuse((x, y), A) - ABB 
                           + feed * (1 - A[x, y])) * delta

                # Do the same for B.
                B2[x, y] = B[x, y] + (D_B * diffuse((x, y), B) + ABB 
                           - (kill + feed) * B[x, y]) * delta

            end # for
        end # for

        # Store the current frame in the cache.
        # The chop is used to store only necessary frames.
        chop = trunc(Int, i/interval - 1/(interval + 1)) + 1

        #== 
        Since arrays are just pointers in julia, a copy
        is created so that when A2/B2 is modified later it
        does not also modify the cache.
        ==#
        cache.A[chop] = deepcopy(A2)
        cache.B[chop] = deepcopy(B2)

        # Update A and B with their new values so that 
        # the next iteration can run.
        A = cache.A[chop]
        B = cache.B[chop]

    end # for

	#== ANIMATION HERE ==#

end # for
```

## Development
A reaction diffusion simulation like this is particularly well suited to being run on the GPU. This is because each individual calculation per cell is very simple, but there are hundreds of thousands of those calculations to do each tick. GPUs have many many individual cores which can perform simple calculations. However, these cores are not very powerful, since this is not a bottleneck for this kind of program a GPU is particularly suited to running it quickly. A CPU has a small number of more powerful cores which makes it less efficient at running a simulation like this. Realising this, and not wanting the simulations to take too long to run, I initially wanted to write this simulation in C++ using OpenFrameworks. However, I did not have enough time to learn C++ or OF; instead I used Julia which runs on the CPU as I have for my other projects (there is some GPU support for Julia but it is in development for the GPU architecture my computer uses).

### Prototype
Very little changed between my initial prototype and final product. Some time was spent making sure the algorithm was efficient and produced GIFs that were at a watchable pace. The most time was spent experimenting with values and starting conditions to see what different effects the program could produce. This is also why varying the Feed and Kill values across the axis was particularly useful - it allowed me to pick on particular areas of interest and expand on them.

*(F = 0.055, K = 0.062) The first simulation I saved:*
![220212 REACT.gif](220212%20REACT.gif)

*(Varying F along x-axis and K along y-axis) See how the starting conditions matter (this simulation misses a lot of the disconnected areas because there were no seeds there. It is also defined by the line of a seed, which is not useful for finding areas of interest):*
![220215 REACT.gif](220215%20REACT.gif)

*(Varying F and K in the same way) This product is defined by the horizontal lines of the seeds.*
![220218 REACT.gif](220218%20REACT.gif)

## Information
Language used: Julia  
Packages imported: ProgressBars, Plots, ColorSchemes

---
# Machine Learning

## Final Product
The neural network follows a simple loop: take the past letters, and predict the next one, then get the cost of the predictions (relative to if they had been 100% correct) and compute the gradients of the parameters with respect to the cost function and update our parameters using gradients descent. The way it does this is emergent, because it uses a collections of nodes which each perform a simple task.

## Explanation
A neural network is a collection of neurons, or nodes, which are attached to some of the nodes in the previous layer via some maths which determines how much of a priority each of those inputs are. Whenever you want an answer from the network, you provide the inputs and read the outputs that are generated by propagating through a number of layers like this. There is nothing inherently complicated or unique (fundamentally at least) about the architecture of a neural network - it's those weights and biases that decide whether or an NN plays chess totally randomly or if it can beat Carlsen.

Unfortunately for us, finding those ideal numbers is really difficult. So that's where training comes in: we initialise the NN with a set of random weights and biases and then forward propagate through it to find what it predicts for each of our sample inputs. Then we can work out how far the guesses were from the real answers and in what directions, using this information we can move the numbers in the NN slightly towards values which will give the correct outputs by moving backwards through the NN or backpropagating.

There are many other ways of training neural nets, including introducing some element of random variation and training two networks to compete against each other to remove the requirement for prelabelled data, but the fundamental principles still apply.

### main.jl

Here the data in `data.txt` is processed to be inputs into our neural net. It takes each line and splits it into 15 character long chunks (with overlap, and allowing blank characters at the beginning).

Take for example:

```
AG v Wilkinson  --  Ahmad Ors
```

First we make each character into the string into an array with one 1 showing the character so `A` would generate an array like

```julia
Int[..., 0, 0, 1, 0, 0, ...]
```

Each value in the array is a specific character, with a special character at the end for stop. 

Lets assume our input length is 5 characters, this is like the NNs memory. To create the first datum for our NN we add 4 (input length -1) blank arrays and the array we just generated (flattening all of these as we go). For the next character, we add 3 (input length -2) blank sets `A` and then the array for `G`. Once we reach 5 characters into the string we remove characters so that it only sees the last 5. For example datum 6 would be `"G v W"`. 

This gives us a lot of data points which can use as training data. So we can pass that (as `DMatrix`) but we need some other parameters first:

```julia
function train_network(layer_dims::Vector{Int64}, DMatrix::Vector{Any}, Y::Vector{Any}; η = 0.001, epochs = 500, seed = 1337, verbose = true)
```

`layer_dims` looks something like this:

```julia
Int64[345, 207, 207, 207, 69]
```

Where each value describes the number of neurons in a layer of the NN (the first and last layers describe the input and output layers respectively). `layer_dims` is a hyperparameter, which means that if we wanted to optimise the NN (and had lots of resources) we could train the NN using some kind of algorithm that worked out how many layers we need (we want to minimise layers where we can, to avoid doing extra calculations, but we don't want to have so few our model can't be accurate). The lengths of the layers are arbitrary but the input layer must match (in our example it's $5 * 69 = 345$) the length of the arrays in `DMatrix` and the output layer must match the length of the arrays in `Y`. 

`Y` is the set of the actual values from our dataset for each item in `DMatrix`. We have been calculating these alongside our inputs so that the two vectors line up. It's the next character in our string (because that's the one we want the NN to guess). 

### train.jl

This contains the function `train_network` which I briefly talked about above which outlines the by which process our NN is going to take to learn. `η` describes the learning (more on this later!). `epochs` tells our code how many times to go through the whole training dataset. `seed` is used instead of the default RNG so we get consistent results (useful to recreate situations). `verbose` tells our code whether it should print to console as it does things.  

First we have to initialise our model, and for this we use `initialise_model(layer_dims, seed)` which can be found in...

### initialise.jl

```julia
function initialise_model(layer_dims, seed)
```
This function creates and returns a dictionary `params` which contains all the initial weights and biases for the model. The biases start at 0 but the weights are randomised to avoid symmetry (we want different parts of our NN to learn to do different things but if the weights all start the same, they would never be different so there might as well only be one neuron in each layer).  

The next thing to do in `train.jl` is to start running our NN! First we propagate forward through the model in...

### forward_propagation.jl

`forward_propagate_model_weights(DMatrix, params)` loops through each layer of the NN and gets the values:

For layer $l$
$$ Z_{l,n} = W_{l,n} * A_{l-1} + b $$
So for each neuron in each layer (except the input layer) we multiply each value in the previous layer by some number (specific to each neuron), and then add a bias (find this in `linear_steps.jl`). We're not quite done there for each layer. We actually need to apply an activation function so our data is remotely usable. This also adds mathematical complexity to the network, which allows it to reach solutions with more detail. The most important activation function is the one in the final layer (in the other layers I primarily use tanh). In the final layer we use a softmax activation function to obtain a values above 0 which sum to 1, like probabilities.
$$ A_{l,n} = \frac {e^{-Z_{l,n}}} {\Sigma e^{-Z}} $$
The greater Z is the closer this will be to 1, and the smaller Z the closer to 0. Usefully it can never fall outside of those bounds (find activation functions in `activations_functions.jl`).

Now that we've iterated through our NN, we need to calculate the cost of our results in...

### cost.jl

For now we are using an implementation of binary cross entropy which says that if you have two functions $p(x)$ and $q(x)$ you can calculate the distance between the results of the two functions essentially like this:
$$ C = q(x) * \log_2 (\frac 1 {p(x)}) $$
The derivation for this is quite complicated but a very readable explanation can be found in [35]. I don't actually know whether or not this the correct cost function for this NN, but it should be close enough that it doesn't matter (in many ways, that is a guiding principle of machine learning).

Our implementation of this looks slightly different:
$$ C = \frac {- \sum ( Y * \log_2 \hat Y + (1 - Y) * \log_2 (1 - \hat Y)) } m $$
Where $m$ is the size of $Y$. This actually gives us the same thing because or actual values ($Y$) are binary. When the actual value is 0 that element of the sum can be simplified to:
$$ 1 * \log_2 (1 - \hat Y) $$
This is almost the same form as above. We've got $1 - \hat Y$ rather than $\frac 1 {1 - \hat Y}$. I've accounted for this by taking that out as a minus (simple log laws). This affects every term so I took it out of the sum. A similar thing is obtained if $Y$ is 1:
$$ 1 * \log_2 \hat Y $$
Next we need to do the really interesting bit: actually learning in...

### backpropagation.jl

We can think about this quite clearly if we imagine a neural network with only one neuron per layer. Essentially we need to take the value we got, and modify our NN to produce a value that's slightly closer to our desired value. Lets take a neuron, $n$, it has a weight, $w$, and bias, $b$. It received an input ($A$) 0, and we want it to output 1. We know the output we got, $Y_n$, and the output we wanted $\hat Y_n$.
$$ \hat Y_n = wA + b $$
It's easy to see how we could optimise one neuron to create the result we want. Substitute for $A$ and our desired $Y_n$. e.g.
$$ 1 = 0w + b $$
Picking random values for w and letting b equal 0 initially (as we do in our NN) currently gives us:
$$ 0*0.74... + 0 = 0 $$
We can clearly see that $w$ does not affect our result in this case, so we can modify $b$ only:
$$ b = b + (Y_n - b) \eta $$
Where $\eta$ is the learning rate.
$$ b = 0 + (1 - 0) * 0.01 = 0.01 $$
So we've modified our value of $b$ to give a slightly better answer than before. We could let $\eta = 1$ then we would correctly guess the right answer for this situation! However, this is called over-optimisation and doesn't actually solve the problem we want it to. Our NN would always be completely overhauled each time we backpropagate to get the ideal result but we would never converge on something which could produce an accurate result from data it hasn't seen before (or even from other data in our dataset, if $\eta = 1$). Even in less extreme cases, over-optimisation is still a big issue. If your NN learns for too long it may learn to fit perfectly to your data and unlearn how to solve the problem which means it would produce less accurate results when it sees data it's never seen before! (This is also why it's important to keep some data for unsupervised tests after supervised training.)

The partial derivatives of each of the variables with respect to $Y$ can be calculated as follows:
$$ \frac {\delta \hat Y} {\delta Y} = - \frac Y {\hat Y} + \frac {1 - Y}{1 - \hat Y} $$
Now that we have this value, we can feed it back into the last layer of neurons to calculate the actual values of $\delta W, \delta b, \delta A$ (or more accurate, the gradient at the tangent to the point on the curve where each parameter lies). This process can then be repeated for each layer backwards (using the $\delta A$ instead of the $\delta \hat Y$ value). So using these calculated gradients for all of the layers, we can update the parameters by moving slightly downwards along each gradient in `gradient_descent.jl`.

### Final steps
The final step is to log all of the parameters and the results of the neural net training.

Then this loop repeats with the new parameters, each loop aims to move the parameters slightly closer to the correct answer each time. After all the loops are done, we can return the important parts of the neural network for use in unlabelled data/in the real world.

## Development
Code snippets can be found in Appendix D.

### Prototype
The prototype was not particularly smartly made and it barely worked. However, it was a very important step in production from where I could improve. Initially I was using reLU functions in all of the hidden layers, which I changed later.

### Modifications
Most of my modifications were: debugging, testing and implementing new ideas. In summary: I made significant progress and learnt a lot but I did not produce a neural network which could really demonstrate completing a task like mimicking human text in all that a complex way (although I can still demonstrate it).  
  
The first change I made was to prepare and begin testing new activation functions: tanh and softmax (for final layer) rather than reLU and sigmoid I was using previously. This involved working out the derivatives for these functions too and implementing those. It took me a longer than expected to work out why these activation functions weren't working as expected or were giving errors.  
  
First off I was getting errors because there was some notation I wasn't using correctly in my code and also I was using Float type values when iterating through the neural network and sometimes those would become so tiny they would become 0 which would cause errors in division (specifically in the softmax function, $\frac {e^{-x}} {\sum e^{-x}}$ where the sum would become 0). I also encountered the opposite problem of dividing by infinity.  
  
To solve these issues I decided to limit the weights and biases to 100/1,000/10,000 (I changed the value I used but these would all do the job) because at that point, once passing through tanh the value will be so close to 1 or -1 it doesn't matter where it's 10,000 or 10,000,000. I also had to use BigFloats which seriously slowed down my code but resolved a number of issues. These values allow julia to manage arbitrary precision in the back end so tanh (for example) will never give a value of 1. and values will never tick over to 0, even when they are much much smaller than a Float64 could handle. I sped up the code a lot by reducing the data size and the network size so this didn't end up being an issue.  
  
I did realise that this was actually a problem common when using activation functions like tanh called Exploding Gradients where the derivatives of the functions become huge because even massive changes only effect the activated output a tiny amount and the net was trying to make some values closer to 0 and some closer to 1.  
  
Then I went through the data and cleaned it up to remove all punctuation (except -) and numbers and upper/lowercase letters (this change made in code not in data.txt). This makes the nets job easier and means less training data should be required to get a good result.  
  
Next I added a logging system which creates files containing all the predictions the net has made and the parameters of the network which I could then analyse later. These would allow me to see if the network was doing its job and observe any obvious problems in the parameters. The logs remain on my computer at home where I have been able to run all the tests I have required.  
  
A significant change I then made was that I fully vectorised the whole process of training. Previously I would forward propagate for one data point, then go calculate the gradients and backpropagate for that one point then move on to the next, but this is inefficient compared to the new approach of forward propagating for all the points and then calculating gradients for all the points then only updating the model weights once. Not only is this more efficient but it also helps the net train because it will have no preference towards the most recently seen data point as it sees every point at once.  

This vectorised process also allowed me to make the program threaded. Using 4 threads rather than one significantly (doubled ish) the speed of the program. Threading essentially takes one process which you want to do a lot of times and distribute each tasks between different threads which will usually run on individual Cores on the CPU. There is overhead managing the threads which means you don't get quite the expected increase in speed (4x off 4 cores) but the speed up is still significant.  

I did some further data cleaning to hopefully confuse the neural net less. (removing random leading - which had appeared and I wasn't sure why but they weren't supposed to be there). This is a hugely important process and not having good data is the main reason neural networks fail to do what you want. They will do exactly as you ask, but that might not be what you want.  

I had been running a lot of (very small) tests throughout this process and I have logs for most of them. It was not able to replicate human names or the structure of these cases. The most important next step for me is to get the Swish activation function working, it seems like it would be very appropriate for the task and is quite exciting.

Later in development I changed the limits on layers using tanh to 3. This seems to be a more normal value in Machine Learning.

## Information
Language used: Julia  
Packages imported: DataFrames, CSV, Dates, ProgressBars

# Conclusion
Throughout the production of my EPQ I have created programs to model emergent behaviour to varying degrees of success. The ACO, Boids and Reaction-diffusion experiments have been the most obviously successful with the Machine Learning perhaps teaching me the most. I set out to learn more about potential applications of emergence in the real world as well as natural situations this method can model simply as the demonstration of how powerful the system is.

Machine Learning and artificial intelligence get thrown around a lot as fancy modern things that no one really understands but that seem to pop up everywhere and, to some extent, this is the case. Even the architect of a complex neural network does not understand what each neuron does or how it came to be, and this means that any bias in datasets can results in AI that doesn't answer the question we wanted it to (but rather the question we asked). For example, dataset bias is what teaches AI screen job applicants to be racist and it can be very hard to eliminate this from our datasets, but that just shows how ingrained racism is in our own society. The datasets are real ones created by humans after all. People really do understand how machine learning works, and the emergent properties that come from stringing a lot of neurons together are the same properties, that, at some level, create our own experience of consciousness. Researching machine learning and building a neural net for this project taught me a lot about who is to blame when a AI goes wrong and about the common pitfalls of a system like this. As with the other models I created, it is a fundamentally simple system (it only seems complex because it benefits from a certain level of mathematical complexity).

The boids, reaction-diffusion, and ant colony models are all about biomimicry (because of course, biology has no central intelligence so of course it would create systems with beautiful emergent properties to solve complex tasks). Watching the boids fly around and enjoying changing the numbers that govern their individual behaviour is an enthralling process in a way I cannot fully describe in this EPQ. The same is true for the reaction-diffusion model. The mere fact that we have described such simple systems that when applied consistently have such surprising and splendid effects is in itself awesome enough to warrant their investigation.

It is clear to me that there is a lot of potential for emergent systems to solve problems in the real world for example, allowing for something akin to central control without any of the limitations which that entails. We are already seeing some of the implications of Machine Learning being realised, for better and for worse, every person who uses a service can get personalised recommendations without a human ever having to figure out a user's likes and dislikes (this is being pushed further and further as we saw the "Youtube Algorithm" which has been succeeded by an even more tuned in algorithm powering Tiktok's recommendations). I am very excited to see where the world takes emergence.

# Glossary
- **Algorithm** - A series of steps which will always find the exact solution to a particular set of problems.
- **Heuristic** - A method of generating solutions to problems which aren't guaranteed to come up the best solution but provide an approximation which can be trusted to some degree. They might use randomness or some algorithm which solves some kind of simpler problem to create an imperfect solution.
- **The Travelling Salesperson Problem** - A problem to find the shortest loop a merchant should take through a set of cities.
- **Exponential** - A function where increasing the input by some value increases the output by a constant factor. They have equations of the form $y = a^x$.
- **Brute Force** - A method of problem solving where all the possible answers are tested to find the best one.
- **Heatmap** - A graph which shows a grid where each grid square has some value represented as a colour (colour legend to the right of the graph).
- **Biomimicry** - Mimicking nature in programs (or as solutions to problems e.g. in architecture).
- **Matrix/Vector** - A Vector is a list of values. A vector can even store vectors! Whereas a matrix is a like a coordinate grid of values (in n-dimensions).
- **Line/Segment** - A line is an infinite line that can be defined by extending the line between two points. A segment is a finite section of a line, e.g. the section between two points.
- **Colinear** - All lie on the same straight line.
- **Euclidean Distance** - Distance between points how we usually perceive it - as measured with a ruler between two points or using Pythagoras' theorem.
- **Initialise** - Create or ready something for use.
- **fps** - Frames Per Second.
- **OpenFrameworks** - An open source framework for writing GPU calculations in C++.
- **Tanh** - An S shaped function with horizontal asymptotes at -1 and 1.
- **Softmax** - A function that takes any values and maps them to a fraction of the total using $e^{-x}$.
- **Datum** - A single piece of data or a part of data.
- **Hyperparameters** - Parameters that affect the whole model and are changed to find the optimal way to run it.
- **Dictionary** - A collection of keys with corresponding values in which keys can be looked up to get/set their values.
- **Debugging** - A process of removing issues (or bugs) from a program.

# Literature Review
While in general I looked to sources that were thorough and gave results I could understand, there is little to be discussed in terms of scientific rigor/due-diligence in these sources. This is because it is generally incredibly clear if a method or calculation is correct, because it can be tested, as I have been doing very frequently throughout creating these models and during the test, it is clear if the change is having the expected effect. This is true in all the models except, perhaps, the machine learning. Where the model could implemented perfectly and we might still not see the expected results.

## Julia
*'Julia: A Fresh Approach to Numerical Computing'*

## Emergence in general
Relevant sources: [13], [22], [26].

## Ant Colony Optimisation
I found some examples of ant colony optimisation: [8], [9]. Initially I saw an application of ACO in [26]. I enjoyed this brief explanation of the problem [12] and referenced [8] and [9] once or twice throughout development to see how the algorithm was structured.

Relevant sources: [8], [9], [12], [26].

## Boids
I initially was given the idea to investigate boids in [25] which led me somehow to this paper [6] and this code/website [10], [11].

Relevant sources: [6], [10], [11], [25].

## Reaction Diffusion
I initially found out about reaction-diffusion simulations through [22]. From there I found [23] with an explanation of the simulation and [34] which demonstrated some of what could be achieved at a large scale.

Relevant sources: [7], [22], [23], [24].

## Machine Learning
I wanted to pursue machine learning for this project when I was reading [28], which is entertaining and informative. This model required a substantial portion of this project's research, but two foundational articles were: [2] and [4]. I also used [1], [24], [27], [32], [35] to provide additional explanation and methods.

Relevant Sources: [1], [2], [4], [24], [27], [28], [32], [35].

# Appendix A
Additional code for Ant Colony Optimisation.

```julia
"""
	gen_points(n::Int, dims::Int) -> Vector{Vector{Float64}}
Returns `n` points in `dims` dimensions.
"""
function gen_points(n::Int, dims::Int)
	return [[rand() for _2 in 1:dims] for _ in 1:n]
end # function

"""
	s(a, b) -> Float64
Returns the euclidean distance between two n-dimensional points, `a` and `b`.
"""
function s(a, b)
	@assert (size(a) == size(b))
	return sum([(a[i] - b[i])^2 for i in 1:length(a)])^(0.5)
end # function
```

```julia
"""
	draw(points, order, [f])
	
Plot the route `order` through `points`.
"""
function draw(points::Vector{Vector{Float64}}, order::Vector{Int}, f = "route.png")

	# Extract the x and y values for plotting.
	x = [points[i][1] for i in order]
	y = [points[i][2] for i in order]

	# Plot.
	plt = plot(x, y, xlims=(0, 1), ylims=(0, 1), marker=:cross, title=string(length(points)) * " points")

	# Save as a .png.
	png(plt, f)

end # function

function draw(ph::Matrix{Float64})

	plt = heatmap(ph)
	png(plt, "route_ph.png")

end # function
```

# Appendix B
A small function is used to update the positions of the boids according to their velocities (after all the other calculations have been done).

```julia
function update(boid::Vector{Float64}, delta)

	# Boid position is the current position (in components)
	# plus the velocity * some time interval.
    return [boid[1] + boid[3] * delta, boid[2] + boid[4] * delta, boid[3], boid[4]]

end
```

A function to calculate the distance between two points. This applies Pythagoras's theorem in n-dimensions.

```julia
function s(a, b)
    @assert (size(a) == size(b))
    return sqrt(sum([(a[d] - b[d])^2 for d in 1:length(a)]))
end # function
```

A function to find the boids nearby to a point (works in any number of dimensions).

```julia
function nearby(c, boids, r)
    return [(b, sqrt((c[1] - b[1])^2 + (c[2] - b[2])^2 )) for b in boids if sqrt((b[1] % w - c[1])^2 + (b[2] - c[2])^2) <= r]
end
```

Additionally, some setup is required:

```julia
using Plots
using ProgressBars

#== FUNCTIONS HERE ==#

# Initialise constants
w = 200
h = 200
steps = 1000
vision = 15
delta = 0.2
n = 50
speedlimit = 10

# Create n boids in random locations including
# just outside of the bounding box.
boids = Vector{Float64}[[rand()*3/2*w-w/4, rand()*3/2*h-h/4, (rand()-0.5) * 5, (rand()-0.5) * 5] for i in 1:n] # x,y,dx,dy 

#== ANIMATION HERE ==#
```

# Appendix C

# Appendix D  
Activation Functions:
```julia
"""
Sigmoid Activation function
"""
function sigmoid(Z::Array{BigFloat})
    A = max.(min.(1 ./ (1 .+ exp.(.-Z)), 1-eps(1.0)), eps(1.0))
    return (A = A, Z = Z)
end # function

"""
ReLU Activation function
"""
function relu(Z)
    A = max.(0, Z)
    return (A = A, Z = Z)
end # function

"""
Softmax Activation function
"""
function softmax(Z::Array{BigFloat})
    A = max.(exp.(Z) ./ ( sum(exp.(Z))), eps(1.0))
    return (A = A, Z = Z)
end # function

"""
Tanh Activation function
"""
function tanhact(Z::Array{BigFloat})
    A = min.(max.(tanh.(Z), -1.0+eps(1.0)), 1.0-eps(1.0))
    return (A = A, Z = Z)
end # function

"""
Swish Activation function
"""
function swish(Z::Array{BigFloat})
    #Z₁ = sigmoid(Z).A
    #A = [Z[i] * Z₁[i] for i in 1:length(Z)]
    A = Z .* sigmoid(Z).A
    return (A = A, Z = Z)
end # function

"""
GELU Activation function
"""
function gelu(Z)
    A = 0.5 * Z * (1 + tanh( sqrt(pi / 2) * (Z + 0.044715 * Z^3) ))
    return (A = A, Z = Z)
end # function
```

Backpropagation:
```julia
include("backward_linear_steps.jl")

"""
compute the gradients (∇) of the parameters (master_cache) of the constructed model with respect to the cost of prediciton (Ŷ) in comparison with actual output (Y).
"""
function back_propagate_model_weights(Ŷ, Y, master_cache)
    # initiate the dictionary to store the gradients for all the components in each layer
    ∇ = Dict()

    L = length(master_cache)
    Y = reshape(Y, size(Ŷ))

    # Partial derivate of the output layer
    δŶ = - (Y / Ŷ) + ((1 .- Y) / (1 .- Ŷ))
    current_cache = master_cache[L]

    # backpropagate on the layer preceeding the output layer
    ∇[string("δW_", (L))], ∇[string("δb_", (L))], ∇[string("δA_", (L-1))] = linear_activation_backwards(δŶ, current_cache, activation_function = "softmax")
    
    # go backward in the layers and compute the partial derivates of each component
    for l = reverse(0:L-2)
        current_cache = master_cache[l+1]
        ∇[string("δW_", (l+1))], ∇[string("δb_", (l+1))], ∇[string("δA_", (l))] = linear_activation_backwards(∇[string("δA_", (l+1))], current_cache, activation_function="tanh")
    end # for

    return ∇
end # function
```

Backward Linear Functions:
```julia
include("backwards_activation_functions.jl")

"""
Partial derivates of the components of linear forward function using the linear output (δZ) and caches of these components (cache).
"""
function linear_backward(δZ, cache)
    # unpack cache
    A_prev, W, b = cache
    m = size(A_prev, 2)

    # partial derivates of each component
    δW = δZ * (A_prev') / m
    δb = sum(δZ, dims = 2) / m
    δA_prev = (W') * δZ

    @assert (size(δA_prev)[1] == size(A_prev)[1])
    @assert (size(δW) == size(W))
    @assert (size(δb)[1] == size(b)[1])

    return δW, δb, δA_prev
end # function

"""
Unpack the linear activated caches (cache) and compute their derivates from the applied activation function.
"""
function linear_activation_backwards(δA, cache; activation_function="relu")
    @assert activation_function ∈ ("sigmoid", "relu", "softmax", "tanh", "swish")

    linear_cache, cache_activation = cache

    if (activation_function == "relu")

        δZ = relu_backwards(δA, cache_activation)

    elseif (activation_function == "sigmoid")

        δZ = sigmoid_backwards(δA, cache_activation)

    elseif (activation_function ==  "tanh")

        δZ = tanh_backwards(δA, cache_activation)

    elseif (activation_function == "softmax")

        δZ = softmax_backwards(δA, cache_activation)
        
    elseif (activation_function == "swish")

        δZ = swish_backwards(δA, cache_activation)

    end # if

    δW, δb, δA_prev = linear_backward(δZ, linear_cache)

    return δW, δb, δA_prev
end # function
```

Backwards Activation Functions:
```julia
include("activation_functions.jl")

"""
Derivative of Sigmoid
"""
function sigmoid_backwards(δA, activated_cache)
    s = sigmoid(activated_cache).A
    s = [max(min(x, 1-(eps(1.0))), eps(1.0)) for x in s]
    δZ = [δA[i] * s[i] * (1 - s[i]) for i in 1:length(s)]

    #δZ = [log(abs( 1 / x - 1 )) for x in δA]
    return δZ
end # function 

"""
Derivative of ReLU
"""
function relu_backwards(δA, activated_cache)
    return [δA[i] * (activated_cache[i] > 0) for i in 1:length(δA)]
end # function

"""
Derivative of Tanh
"""
function tanh_backwards(δA, activated_cache)
    return δA[i] .* (1 .- (tanh(activated_cache[i])).^2)
end # function

"""
Derivative of Softmax
"""
function softmax_backwards(δA, activated_cache)
    return δA * ( - exp.(activated_cache) .* sum(exp.(activated_cache)) )
end # function

"""
Derivative of Swish
"""
function swish_backwards(δA, activated_cache)
    # β = 1
    return δA .* (swish(activated_cache).A .+ sigmoid(activated_cache).A .* (1 .- swish(activated_cache).A))
end # function
```

Check Accuracy:
```julia
"""
Check the accuracy between predicted values (Ŷ) and the true value (Y).
"""
function assess_accuracy(Ŷ, Y)
    @assert size(Ŷ)[1] == size(Y)[1]
    return Ŷ[findmax(Y)[2]]
end # function
```

Cost:
```julia
"""
Binary Cross Entropy calc.
"""
function calculate_cost(Ŷ, Y)
    m = size(Y, 2)
    epsilon = eps(1.0)

    # Deal with log(0)
    Ŷ_new = [min(i, epsilon) for i in [max(i, epsilon) for i in Ŷ]]

    # using hardcoded 29 as number of characters
    #cost = -sum(Y .* log2.(Ŷ_new) + ((1 + 2/29) .- Y) .* log2.((1 + 2/29) .- Ŷ_new)) /m
    cost = -sum([ Y[i] == 1 ? log2(abs(Ŷ_new[i])) : log2(abs(Ŷ_new[i] + 27/29)) for i in 1:length(Y)]) /m
    #cost = -log2(Ŷ_new[findmax(Y)[2]])
    return cost
end # function
```

Forward Propagation:
```julia
include("linear_steps.jl")

"""
Iterate through the layers in the net to get a prediction.
"""
function forward_propagate_model_weights(DMatrix, parameters)
    master_cache = []
    A = DMatrix
    L = trunc(Int, length(parameters) / 2)

    # Forawrd propagate until the last layer
    for l = 1:(L-1)
        A_prev = A
        A, cache = linear_forward_activate(A_prev, parameters[string("W_", (l))], parameters[string("b_", (l))], activation_function = "tanh")
        # Debugging println statements
        #==if l == L-1
            println(string(l, " : ", A[1:2]))
        end # if==#
        push!(master_cache, cache)
    end # for

    # For the final layer (outputs)
    Ŷ, cache = linear_forward_activate(A, parameters[string("W_", (L))], parameters[string("b_", (L))], activation_function = "softmax")
    # Make sure the outputs are within bounds too
    push!(master_cache, cache)

    return Ŷ, master_cache
end # function
```

Gradient Descent:
```julia
"""
Update the parameters of the model using the gardients (∇) and the learning rate (η).
"""
function update_model_weights(parameters, ∇, η)
    L = trunc(Int, length(parameters) / 2)

    # update the parameters (weights and biases) for all the layers
    for l = 1:L
        parameters[string("W_", l)] = min.(max.(parameters[string("W_", l)] - η .* ∇[string("δW_", l)], -100), 100)
        # try not editign biases, and get swish working
        parameters[string("b_", l)] = min.(max.(parameters[string("b_", l)] - η .* ∇[string("δb_", l)], -100), 100)

        # Debugging printlns
        #println(∇[string("δW_", l)][1:5])
        #println(parameters[string("W_", l)][1:5])
    end # for

    return parameters
end # function
```

Initialise:
```julia
using StableRNGs

"""
Function to initialise the parameters of the desired network.
"""
function initialise_model(layer_dims::Vector{Int}, seed::Any)
    params = Dict()

    for l = 2:length(layer_dims)
        params[string("W_", (l-1))] = ( rand( StableRNG(seed), layer_dims[l], layer_dims[l-1] ) .- 0.5) * 200#* sqrt(layer_dims[l-1])
        params[string("b_", (l-1))] = zeros(layer_dims[l], 1)
    end # for

    return params
end
```

Linear steps:
```julia
include("activation_functions.jl")

"""
Make a linear forward calc.
"""
function linear_forward(A, W, b)
    # Make a linear forward and return the inputs as cache
    Z = BigFloat.((W * A) .+ b)
    cache = (A, W, b)

    @assert size(Z) == (size(W, 1), size(A, 2))

    return (Z = Z, cache = cache)
end # function

"""
Make a forward activation
"""
function linear_forward_activate(A_prev, W, b; activation_function = "tanh")
    @assert activation_function ∈ ("sigmoid", "relu", "softmax", "tanh", "swish")
    Z, linear_cache = linear_forward(A_prev, W, b)

    if activation_function == "sigmoid"
        A, activation_cache = sigmoid(Z)
    elseif activation_function == "relu"
        A, activation_cache = relu(Z)
    elseif activation_function == "softmax"
        A, activation_cache = softmax(Z)
    elseif activation_function == "tanh"
        A, activation_cache = tanhact(Z)
    elseif activation_function == "swish"
        A, activation_cache = swish(Z)
    end # if

    cache = (linear_step_cache = linear_cache, activation_step_cache = activation_cache)

    @assert size(A)[1] == size(W, 1)[1]

    return A, cache
end # function
```

Train:
```julia
using DataFrames, CSV, Dates, ProgressBars

include("initialise.jl")
include("forward_propagation.jl")
include("cost.jl")
include("check_accuracy.jl")
include("backpropagation.jl")
include("gradient_descent.jl")

"""
Train the network using the desired architecture by matching the training inputs (DMatrix) to their
corresponding outputs (Y) over some number of iterations (epochs) and a learning rate (η).
"""
function train_network(layer_dims::Vector{Int64}, DMatrix::Vector{Vector{Float64}}, Y::Vector{Vector{Float64}}, mapping::String; η = 0.001, epochs = 500, seed = 758907)

    η_first = η
    costs = BigFloat[BigFloat(0.0) for x in 1:length(DMatrix) * epochs]
    iters = Int[trunc((x-1) / length(DMatrix))+1 for x in 1:length(DMatrix) * epochs]
    confidence = BigFloat[BigFloat(0.0) for x in 1:length(DMatrix) * epochs]
    YvŶ = Vector{Char}[Char[' ' for x in 1:length(DMatrix) * epochs], Char[' ' for x in 1:length(DMatrix) * epochs]]
    mapping *= "#"

    # initialise
    params = initialise_model(layer_dims, seed)

    # LOGGING
    d = string(Dates.now())[1:13] * "-" * string(Dates.now())[15:16]

    # train
    for i = 1:epochs

        Ŷ = Any[[] for x in 1:length(DMatrix)]
        caches = [[] for x in 1:length(DMatrix)]

        iter = ProgressBar(1:length(DMatrix))
        set_description(iter, "Epoch $i 1/6 Forward Propagating")

        Threads.@threads for datum in iter
            Ŷ[datum], caches[datum] = forward_propagate_model_weights(DMatrix[datum], params)
        end # for

        set_description(iter, "Epoch $i 2/6 Calculating Costs") # I sorta modified this step...

        Threads.@threads for datum in iter
            costs[(i-1) * length(DMatrix) + datum] = calculate_cost(Ŷ[datum], Y[datum])
        end # for

        set_description(iter, "Epoch $i 3/6 Calculating Confidence")

        Threads.@threads for datum in iter
            confidence[(i-1) * length(DMatrix) + datum] = assess_accuracy(Ŷ[datum], Y[datum])
        end # for

        set_description(iter, "Epoch $i 4/6 Calculating Gradients")

        ∇ = Any[missing for x in 1:length(DMatrix)]
        Threads.@threads for datum in iter
            ∇[datum] = back_propagate_model_weights(Ŷ[datum], Y[datum], caches[datum])
        end # for

        iter = ProgressBar(1:length(collect(keys(params))))
        set_description(iter, "Epoch $i 5/6 Updating Model Params")
        ∇_avg = Dict()

        for x in iter
            ∇_avg[string("δ", collect(keys(params))[x])] = sum([δ[string("δ", collect(keys(params))[x])] for δ in ∇]) / length(∇)
        end # for

        params = update_model_weights(params, ∇_avg, η)

        iter = ProgressBar(1:length(DMatrix))
        set_description(iter, "Epoch $i 6/6 Logging calcs")

        for datum in iter
            YvŶ[1][(i-1) * length(DMatrix) + datum] = mapping[findmax(Y[datum])[2]]
            YvŶ[2][(i-1) * length(DMatrix) + datum] = mapping[findmax(Ŷ[datum])[2][1]]
        end # for

        open("case_names/logs/$d LOG.csv", "w") do io
            CSV.write(io, DataFrame([iters, costs, confidence, [(YvŶ[1][x] == YvŶ[2][x]) for x in 1:length(YvŶ[1])], YvŶ[1], YvŶ[2]], :auto), header=["Iteration", "Cost", "Accuracy", "Correct", "Answer", "Guess"])
        end # do
        open("case_names/logs/$d PAR.csv", "w") do io
            CSV.write(io, DataFrame([collect(values(params))], :auto), header=collect(keys(params)))
        end # do

    end # for

    return (cost = costs, iterations = iters, confidence = confidence, parameters = params)

end # function
```

# Bibliography
1. _12 Types of Neural Networks Activation Functions: How to Choose?_ (no date). Available at: [https://www.v7labs.com/blog/neural-networks-activation-functions](https://www.v7labs.com/blog/neural-networks-activation-functions) (Accessed: 3 December 2021).

2. Andrade, X.G. (2019) _How to Build a Deep Neural Network from Scratch with Julia._, _Medium_. Available at: [https://medium.datadriveninvestor.com/how-to-build-a-deep-neural-network-from-scratch-with-julia-862116a194c](https://medium.datadriveninvestor.com/how-to-build-a-deep-neural-network-from-scratch-with-julia-862116a194c) (Accessed: 30 November 2021).

3. Bezanson, J. _et al._ (2017) ‘Julia: A Fresh Approach to Numerical Computing’, _SIAM Review_, 59(1), pp. 65–98. doi:[10.1137/141000671](https://doi.org/10.1137/141000671).

4. Brenyah, B. (2020) _How To Build An Artificial Neural Network From Scratch In Julia_, _Medium_. Available at: [https://towardsdatascience.com/how-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8](https://towardsdatascience.com/how-to-build-an-artificial-neural-network-from-scratch-in-julia-c839219b3ef8) (Accessed: 30 November 2021).

5. _Carbon_ (no date). Available at: [https://carbon.now.sh/](https://carbon.now.sh/) (Accessed: 18 April 2022).

6. _Craig Reynolds: Flocks, Herds, and Schools: A Distributed Behavioral Model_ (no date). Available at: [http://www.cs.toronto.edu/~dt/siggraph97-course/cwr87/](http://www.cs.toronto.edu/~dt/siggraph97-course/cwr87/) (Accessed: 4 March 2022).

7. Cristina, S. (2021) ‘A Gentle Introduction to the Laplacian’, _Machine Learning Mastery_, 6 August. Available at: [https://machinelearningmastery.com/a-gentle-introduction-to-the-laplacian/](https://machinelearningmastery.com/a-gentle-introduction-to-the-laplacian/) (Accessed: 23 April 2022).

8. Dent, G. (2021) _JS ACO_. Available at: [https://github.com/GordyD/js-aco](https://github.com/GordyD/js-aco) (Accessed: 23 April 2022).

9. Dodd, L. (2020) _Implementation of Ant Colony Optimisation for the Travelling Salesman Problem_. Available at: [https://github.com/lukedodd/ant-tsp](https://github.com/lukedodd/ant-tsp) (Accessed: 23 April 2022).

10. Eater, B. (2022) _Boids algorithm demonstration_. Available at: [https://github.com/beneater/boids/blob/86b4cb9896f43d598867b7d58986210ba21f03de/boids.js](https://github.com/beneater/boids/blob/86b4cb9896f43d598867b7d58986210ba21f03de/boids.js) (Accessed: 2 March 2022).

11. Eater, B. (no date) _Boids algorithm_. Available at: [https://eater.net/boids](https://eater.net/boids) (Accessed: 2 March 2022).

12. Eddie Woo (2015) _The Travelling Salesman (3 of 3: Ant Colonisation Heuristic)_. Available at: [https://www.youtube.com/watch?v=2jncD54ryGs](https://www.youtube.com/watch?v=2jncD54ryGs) (Accessed: 8 April 2022).

13. _Emergent Intelligence - Chetan Surpur_ (no date). Available at: [https://chetansurpur.com/blog/2013/08/emergent-intelligence.html](https://chetansurpur.com/blog/2013/08/emergent-intelligence.html) (Accessed: 30 November 2021).

14. _Home · Plots_ (no date). Available at: [https://docs.juliaplots.org/latest/](https://docs.juliaplots.org/latest/) (Accessed: 18 April 2022).

15. ‘How to check if two given line segments intersect?’ (2013) _GeeksforGeeks_, 10 July. Available at: [https://www.geeksforgeeks.org/check-if-two-given-line-segments-intersect/](https://www.geeksforgeeks.org/check-if-two-given-line-segments-intersect/) (Accessed: 23 April 2022).

16. _Join GIF images - Free online tool_ (no date). Available at: [https://joingif.imageonline.co/](https://joingif.imageonline.co/) (Accessed: 18 April 2022).

17. _learning | openFrameworks_ (no date). Available at: [https://openframeworks.cc/learning/#how_tos](https://openframeworks.cc/learning/#how_tos) (Accessed: 28 February 2022).

18. Müller-Komorowska, D. (2020) ‘Animations with Julia’, _Scientific Programming Blog_, 20 August. Available at: [https://danielmuellerkomorowska.com/2020/08/20/animations-with-julia/](https://danielmuellerkomorowska.com/2020/08/20/animations-with-julia/) (Accessed: 19 January 2022).

19. neuralthreads (2021) ‘Tanh function — “S” shaped function similar to the Sigmoid function’, _Medium_, 3 December. Available at: [https://medium.com/@neuralthreads/tanh-function-s-shaped-function-similar-to-the-sigmoid-function-5c8d76dca4bb](https://medium.com/@neuralthreads/tanh-function-s-shaped-function-similar-to-the-sigmoid-function-5c8d76dca4bb) (Accessed: 3 December 2021).

20. _ofBook - C++ Language Basics_ (no date). Available at: [https://openframeworks.cc/ofBook/chapters/cplusplus_basics.html#conclusion](https://openframeworks.cc/ofBook/chapters/cplusplus_basics.html#conclusion) (Accessed: 28 February 2022).

21. _ofBook/unabridged.md at master · openframeworks/ofBook_ (no date) _GitHub_. Available at: [https://github.com/openframeworks/ofBook](https://github.com/openframeworks/ofBook) (Accessed: 28 February 2022).

22. _physarum - Sage Jenson_ (no date). Available at: [https://cargocollective.com/sagejenson/physarum](https://cargocollective.com/sagejenson/physarum) (Accessed: 11 February 2022).

23. _Reaction-Diffusion Tutorial_ (no date). Available at: [http://www.karlsims.com/rd.html](http://www.karlsims.com/rd.html) (Accessed: 11 February 2022).

24. Refaeli, D. (no date) _Sigmoid, Softmax and their derivatives_. Available at: [https://themaverickmeerkat.com/2019-10-23-Softmax/](https://themaverickmeerkat.com/2019-10-23-Softmax/) (Accessed: 3 December 2021).

25. Sebastian Lague (2019) _Coding Adventure: Boids_. Available at: [https://www.youtube.com/watch?v=bqtqltqcQhw](https://www.youtube.com/watch?v=bqtqltqcQhw) (Accessed: 4 March 2022).

26. Sebastian Lague (2021) _Coding Adventure: Ant and Slime Simulations_. Available at: [https://www.youtube.com/watch?v=X-iSQQgOd1A](https://www.youtube.com/watch?v=X-iSQQgOd1A) (Accessed: 11 February 2022).

27. Serengil, S. (2017) ‘Hyperbolic Tangent as Neural Network Activation Function’, _Sefik Ilkin Serengil_, 28 January. Available at: [https://sefiks.com/2017/01/29/hyperbolic-tangent-as-neural-network-activation-function/](https://sefiks.com/2017/01/29/hyperbolic-tangent-as-neural-network-activation-function/) (Accessed: 3 December 2021).

28. Shane, J. (2020) _You look like a thing and I love you_. London: Wildfire.

29. softologyblog (2020) ‘Ant Colony Simulations’, _Softology’s Blog_, 20 March. Available at: [https://softologyblog.wordpress.com/2020/03/21/ant-colony-simulations/](https://softologyblog.wordpress.com/2020/03/21/ant-colony-simulations/) (Accessed: 19 January 2022).

30. _StatsBase.jl Documentation · StatsBase.jl_ (no date). Available at: [https://juliastats.org/StatsBase.jl/stable/](https://juliastats.org/StatsBase.jl/stable/) (Accessed: 18 April 2022).

31. The Manim Community Developers (2021) _Manim – Mathematical Animation Framework_. Available at: [https://www.manim.community/](https://www.manim.community/) (Accessed: 30 November 2021).

32. Tung.M.Phung (no date) ‘Sigmoid, tanh activations and their loss of popularity’, _Tung M Phung’s Blog_. Available at: [https://tungmphung.com/sigmoid-tanh-activations-and-their-loss-of-popularity/](https://tungmphung.com/sigmoid-tanh-activations-and-their-loss-of-popularity/) (Accessed: 3 December 2021).

33. ‘Understanding Multiple Neighborhood Cellular Automata - Slackermanz’ (2021), 23 May. Available at: [https://slackermanz.com/understanding-multiple-neighborhood-cellular-automata/](https://slackermanz.com/understanding-multiple-neighborhood-cellular-automata/) (Accessed: 14 February 2022).

34. _U-Skate World, an Instance of the Gray-Scott System at MROB_ (no date). Available at: [http://mrob.com/pub/comp/xmorphia/uskate-world.html](http://mrob.com/pub/comp/xmorphia/uskate-world.html) (Accessed: 14 February 2022).

35. _Visual Information Theory -- colah’s blog_ (no date). Available at: [https://colah.github.io/posts/2015-09-Visual-Information/](https://colah.github.io/posts/2015-09-Visual-Information/) (Accessed: 30 November 2021).